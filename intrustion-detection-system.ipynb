{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b76caf6",
   "metadata": {},
   "source": [
    "### **IMPORTANT NOTE: The following code and explanation was generated via ChatGPT o1-preview model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a60cfd",
   "metadata": {},
   "source": [
    "# Step 1: Import Necessary Libraries\n",
    "\n",
    "In your Jupyter Notebook, start by importing the libraries we'll need. You can find the libraries we are installing below or in the `README.md`\n",
    "\n",
    "**Explanation**:\n",
    "- **pandas**: For data manipulation and analysis.\n",
    "- **numpy**: For numerical computations.\n",
    "- **os**: For interacting with the operating system.\n",
    "- **matplotlib.pyplot** and **seaborn**: For data visualization.\n",
    "- **sklearn**: For machine learning algorithms and evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "816a5cf1-b89d-4e39-b76b-9df46d1f3455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Add any other imports you may have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd64472e",
   "metadata": {},
   "source": [
    "# Step 2: Load and Combine Dataset Files\n",
    "\n",
    "We load all CSV files from the dataset/ folder and combine them into one DataFrame for analysis.\n",
    "\n",
    "1. **Import Libraries**: Import necessary modules for file handling and data manipulation.\n",
    "2. **Locate CSV Files**: Find all CSV files in the dataset/ directory.\n",
    "3. **Load Data**: Read each CSV file into a DataFrame, clean column names, and store them in a list.\n",
    "4. **Combine Data**: Merge all DataFrames into a single DataFrame data.\n",
    "5. **Confirm Loading**: Print the shapes to verify that data has been loaded correctly.\n",
    "\n",
    "Now, we have all our data in one place, ready for preprocessing and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b178ec18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your dataset folder\n",
    "data_path = 'dataset/'\n",
    "\n",
    "# Get a list of all CSV files in the dataset folder\n",
    "csv_files = glob.glob(os.path.join(data_path, '*.csv'))\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file, encoding='utf-8')  # Adjust encoding if necessary\n",
    "    # Strip whitespace from column names\n",
    "    df.columns = df.columns.str.strip()\n",
    "    dataframes.append(df)\n",
    "    print(f\"Loaded {file} with shape {df.shape}\")\n",
    "\n",
    "# Combine all DataFrames\n",
    "data = pd.concat(dataframes, ignore_index=True)\n",
    "print(f\"Combined DataFrame shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bc0ba6",
   "metadata": {},
   "source": [
    "# Step 3: Clean the `Label` Column\n",
    "\n",
    "We clean the 'Label' column to ensure our labels are consistent:\n",
    "\n",
    "#### Replace Unwanted Characters:\n",
    "- Some labels might contain an unidentified character '�' due to encoding issues.\n",
    "- We replace '�' with a hyphen '-' to fix these labels.\n",
    "\n",
    "#### Remove Extra Spaces:\n",
    "- We strip any leading or trailing whitespace from the labels.\n",
    "- This ensures there are no hidden spaces that could cause problems later.\n",
    "\n",
    "By cleaning the 'Label' column, we make sure that all labels are properly formatted for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b3c8cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the unidentified character '�' with a hyphen '-' in the 'Label' column\n",
    "data['Label'] = data['Label'].str.replace('�', '-', regex=False)\n",
    "\n",
    "# Optionally, strip any leading/trailing whitespace from the 'Label' column\n",
    "data['Label'] = data['Label'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398cd8fd",
   "metadata": {},
   "source": [
    "# Step 4: Explore the Data\n",
    "### 4.1 View the First Few Rows\n",
    "\n",
    "Preview the Data:\n",
    "- `data.head()`: Displays the first five rows of the dataset.\n",
    "- Purpose: Gives us a quick look at the data structure, column names, and some sample values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f202743",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7db4b8c",
   "metadata": {},
   "source": [
    "### 4.2 Get DataFrame Information\n",
    "Dataset Summary:\n",
    "- `data.info()`: Provides a summary of the dataset.\n",
    "    - Shows the number of entries (rows) and columns.\n",
    "    - Displays the data type of each column.\n",
    "    - Indicates the number of non-null values in each column.\n",
    "- Purpose: Helps us understand the overall structure of the data and identify any columns with missing values or incorrect data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d7ab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a216d232",
   "metadata": {},
   "source": [
    "### 4.3 Check for Missing Values\n",
    "Check for Missing Values:\n",
    "- `data.isnull().sum()`: Calculates the number of missing values in each column.\n",
    "- `print(missing_values)`: Outputs the count of missing values per column.\n",
    "- Purpose: Identifies columns that may need data cleaning or imputation due to missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2b3646",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = data.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4830c934",
   "metadata": {},
   "source": [
    "# Step 5: Preprocess the Data\n",
    "### 5.1 Convert Non-Numeric Columns to Numeric\n",
    "\n",
    "We start preprocessing by finding any non-numeric columns in our data:\n",
    "\n",
    "- Find Non-Numeric Columns:\n",
    "    - Use `data.select_dtypes(include=['object']).columns` to list columns with data type `object`.\n",
    "    - These are usually categorical or text features that need to be converted to numbers for our machine learning models.\n",
    "- Print the Columns:\n",
    "    - We display the names of these non-numeric columns to decide how to handle them next.\n",
    "\n",
    "Identifying these columns helps us prepare for encoding them into numeric format, ensuring our data is ready for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26a6618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify non-numeric columns\n",
    "non_numeric_cols = data.select_dtypes(include=['object']).columns\n",
    "print(\"Non-numeric columns:\", non_numeric_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c79783",
   "metadata": {},
   "source": [
    "### 5.2 Encode Categorical Variables\n",
    "#### 5.2.1 Encode the Target Variable\n",
    "\n",
    "Before encoding the target variable, let's check the unique classes present in the 'Label' column.\n",
    "\n",
    "Explanation:\n",
    "- View Unique Labels:\n",
    "    - We print all the unique values in the `Label` column to see the different classes in our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9aacce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values in the Label column\n",
    "print(data['Label'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6639c70",
   "metadata": {},
   "source": [
    "#### 5.2.2 Encode Labels for Binary Classification\n",
    "\n",
    "In this step, we'll map all attack types to `Attack` and benign traffic to `Benign`. We'll then encode these labels into numerical values.\n",
    "\n",
    "The Label Values are as follows:\n",
    "```\n",
    "    ['BENIGN' 'Infiltration' 'Bot' 'PortScan' 'DDoS' 'FTP-Patator',\n",
    "     'SSH-Patator' 'DoS slowloris' 'DoS Slowhttptest' 'DoS Hulk', \n",
    "     'DoS GoldenEye' 'Heartbleed' 'Web Attack - Brute Force',\n",
    "     'Web Attack - XSS' 'Web Attack - Sql Injection']\n",
    "```\n",
    "\n",
    "Explanation:\n",
    "- We create a new column `Label_binary` where all attack types are mapped to `Attack` and benign traffic to `Benign`.\n",
    "- `LabelEncoder` is used to convert these categorical labels into numerical values (0 and 1).\n",
    "- We print the label mapping to verify that `Attack` and `Benign` are correctly encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6839d165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels for binary classification\n",
    "data['Label_binary'] = data['Label'].apply(lambda x: 'Benign' if x == 'BENIGN' else 'Attack')\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "data['Label_encoded'] = le.fit_transform(data['Label_binary'])\n",
    "\n",
    "# Display label encoding mapping\n",
    "label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(\"Label Encoding Mapping:\")\n",
    "for label, encoding in label_mapping.items():\n",
    "    print(f\"{label}: {encoding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa05e548",
   "metadata": {},
   "source": [
    "### 5.3 Prepare Features (X) and Target (y)\n",
    "\n",
    "We'll separate our dataset into features and target variables.\n",
    "\n",
    "Explanation:\n",
    "- `X` contains all the features used for training, excluding the original and encoded labels.\n",
    "- `y` is our target variable containing the encoded labels (`0` for **Attack**, `1` for **Benign**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4be95c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features (drop unnecessary columns)\n",
    "X = data.drop(['Label', 'Label_binary', 'Label_encoded'], axis=1, errors='ignore')\n",
    "\n",
    "# Target variable\n",
    "y = data['Label_encoded']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71773bf",
   "metadata": {},
   "source": [
    "### 5.4 Handle Non-Numeric Features\n",
    "\n",
    "We need to ensure all features are numeric.\n",
    "\n",
    "Explanation:\n",
    "- We check for any non-numeric columns in `X`.\n",
    "- If non-numeric columns are found, we apply one-hot encoding to convert them into numeric format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a238de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify non-numeric columns\n",
    "non_numeric_cols = X.select_dtypes(include=['object']).columns\n",
    "print(\"Non-numeric columns:\", non_numeric_cols.tolist())\n",
    "\n",
    "# Encode non-numeric features\n",
    "if len(non_numeric_cols) > 0:\n",
    "    X = pd.get_dummies(X, columns=non_numeric_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139b541a",
   "metadata": {},
   "source": [
    "### 5.5 Split Data into Training and Testing Sets\n",
    "\n",
    "We'll split our data into training and testing sets to evaluate model performance.\n",
    "\n",
    "Explanation:\n",
    "- We use `train_test_split` to split the data.\n",
    "    - `test_size=0.2` reserves 20% of the data for testing and 80% for training\n",
    "    - `stratify=y` ensures the class distribution is consistent in both training and testing sets.\n",
    "    - `random_state=42` sets a seed for random number generation to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3152a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data with stratification to maintain class balance\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df30b89c",
   "metadata": {},
   "source": [
    "### 5.6 Handling Infinite and Missing Values\n",
    "\n",
    "We need to replace infinite values and handle any missing data in our features.\n",
    "\n",
    "Explanation:\n",
    "- Infinite values are replaced with `NaN` to handle them appropriately.\n",
    "- We check for missing values in `X_train` and `X_test`.\n",
    "- `SimpleImputer` is used to fill missing values with the mean of each feature, calculated from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a73f31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "X_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Check for NaN values\n",
    "print(\"Checking for NaN values in X_train and X_test:\")\n",
    "print(f\"X_train contains NaN values: {X_train.isnull().values.any()}\")\n",
    "print(f\"X_test contains NaN values: {X_test.isnull().values.any()}\")\n",
    "\n",
    "# Impute missing values with the mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Fit on X_train and transform both X_train and X_test\n",
    "X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0366b6",
   "metadata": {},
   "source": [
    "# Step 6: Feature Scaling\n",
    "We scale the features to normalize the data.\n",
    "\n",
    "Explanation:\n",
    "- `StandardScaler` standardizes features by removing the mean and scaling to unit variance.\n",
    "- We fit the scaler on `X_train` and transform both `X_train` and `X_test` to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e878cfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c77e935",
   "metadata": {},
   "source": [
    "# Step 7: Model Evaluation\n",
    "### Baseline with no adjustments\n",
    "We train a baseline Random Forest model without any adjustments to see how it performs:\n",
    "\n",
    "- Train the Model:\n",
    "    - Create a `RandomForestClassifier` with 100 trees.\n",
    "    - Fit the model to the scaled training data.\n",
    "\n",
    "- Make Predictions:\n",
    "    - Predict the labels for the test data.\n",
    "\n",
    "- Evaluate Performance:\n",
    "    - Accuracy: Calculate the overall accuracy of the model.\n",
    "    - Classification Report: Get precision, recall, F1-score, and support for each class.\n",
    "    - Confusion Matrix: See the breakdown of correct and incorrect predictions for 'Attack' and 'Benign'.\n",
    "\n",
    "This gives us a baseline to assess whether further improvements are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedddfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Model: RandomForestClassifier without class weights\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Train the baseline model\n",
    "baseline_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "baseline_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_baseline = baseline_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred_baseline)\n",
    "print(f\"Baseline Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"Baseline Model Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_baseline, target_names=['Attack', 'Benign']))\n",
    "\n",
    "print(\"Baseline Model Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_baseline))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90852fd7",
   "metadata": {},
   "source": [
    "### Baseline with class weights\n",
    "We modify the Random Forest model to give more attention to the minority class:\n",
    "\n",
    "- Adjust Class Weights:\n",
    "    - Use `class_weight='balanced'` to automatically balance class weights.\n",
    "    - Helps the model focus more on detecting `Attack` instances.\n",
    "\n",
    "- Train, Predict, and Evaluate:\n",
    "    - Fit the model to the training data.\n",
    "    - Make predictions on the test data.\n",
    "    - Evaluate using accuracy, classification report, and confusion matrix.\n",
    "\n",
    "This step helps us assess whether class weighting improves model performance compared to the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e301f4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with class_weight='balanced'\n",
    "model_class_weight = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
    "model_class_weight.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_class_weight = model_class_weight.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred_class_weight)\n",
    "print(f\"Model with Class Weight Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"Model with Class Weight Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_class_weight, target_names=['Attack', 'Benign']))\n",
    "\n",
    "print(\"Model with Class Weight Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_class_weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc29b74",
   "metadata": {},
   "source": [
    "### Oversample Minority Class using SMOTE\n",
    "\n",
    "We address class imbalance by oversampling the minority class ('Attack') using **SMOTE** (Synthetic Minority Oversampling Technique).\n",
    "\n",
    "Explanation:\n",
    "- `SMOTE` generates synthetic samples of the minority class to balance the dataset.\n",
    "- `sampling_strategy='auto'` balances all classes to the number of samples in the majority class.\n",
    "- We apply `SMOTE` only to the training data to avoid data leakage.\n",
    "- We print class distributions before and after resampling to verify the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2df27fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Define the SMOTE object\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Resample the training data\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(\"After SMOTE oversampling:\")\n",
    "print(f\"Original y_train distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Resampled y_train distribution: {np.bincount(y_train_resampled)}\")\n",
    "\n",
    "# Train the model on resampled data\n",
    "model_smote = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model_smote.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_smote = model_smote.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred_smote)\n",
    "print(f\"Model with SMOTE Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"Model with SMOTE Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_smote, target_names=['Attack', 'Benign']))\n",
    "\n",
    "print(\"Model with SMOTE Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_smote))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d8e86f",
   "metadata": {},
   "source": [
    "### Logistic Regression Evaluation\n",
    "We test Logistic Regression as an alternative model:\n",
    "\n",
    "- Set Up the Model:\n",
    "    - Use LogisticRegression with:\n",
    "        - `max_iter=1000` to allow more iterations for convergence.\n",
    "        - `class_weight='balanced'` to handle class imbalance.\n",
    "        - `random_state=42` for consistent results.\n",
    "\n",
    "- Train the Model:\n",
    "    - Fit the logistic regression model to the scaled training data.\n",
    "\n",
    "- Make Predictions:\n",
    "    - Predict labels for the test data.\n",
    "\n",
    "- Evaluate Performance:\n",
    "    - Accuracy: Check the overall correctness of the model.\n",
    "    - Classification Report: Get detailed metrics like precision and recall for 'Attack' and 'Benign'.\n",
    "    - Confusion Matrix: See how well the model distinguishes between the classes.\n",
    "\n",
    "By comparing Logistic Regression to our previous models, we can see which algorithm performs best on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c712cdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize and train the model\n",
    "model_logreg = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)\n",
    "model_logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_logreg = model_logreg.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred_logreg)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"Logistic Regression Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_logreg, target_names=['Attack', 'Benign']))\n",
    "\n",
    "print(\"Logistic Regression Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_logreg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c96c87",
   "metadata": {},
   "source": [
    "### XGBoost Evaluation\n",
    "We train an XGBoost model to classify network traffic, adjusting for class imbalance:\n",
    "\n",
    "- Handle Class Imbalance:\n",
    "    - Calculate `scale_pos_weight` by finding the ratio of `Benign` to `Attack` instances in the training data.\n",
    "    - This tells the model to pay more attention to the minority class.\n",
    "\n",
    "- Train the XGBoost Model:\n",
    "    - Initialize `XGBClassifier` with the calculated `scale_pos_weight`.\n",
    "    - Train the model on the scaled training data.\n",
    "\n",
    "- Make Predictions:\n",
    "    - Use the trained model to predict labels for the test data.\n",
    "\n",
    "- Evaluate Performance:\n",
    "    - Accuracy: Measure how often the model's predictions are correct.\n",
    "    - Classification Report: Get detailed metrics like precision and recall for `Attack` and `Benign`.\n",
    "    - Confusion Matrix: See the breakdown of correct and incorrect predictions for each class.\n",
    "\n",
    "By adjusting for class imbalance, we aim to improve the model's ability to detect attacks effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9edcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the XGBoost classifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Calculate the scale_pos_weight parameter\n",
    "from collections import Counter\n",
    "counter = Counter(y_train)\n",
    "ratio = counter[1] / counter[0]\n",
    "print(f\"Scale_pos_weight ratio: {ratio}\")\n",
    "\n",
    "# Initialize and train the model\n",
    "model_xgb = XGBClassifier(scale_pos_weight=ratio, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "model_xgb.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgb = model_xgb.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred_xgb)\n",
    "print(f\"XGBoost Classifier Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"XGBoost Classifier Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb, target_names=['Attack', 'Benign']))\n",
    "\n",
    "print(\"XGBoost Classifier Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_xgb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70b0a84",
   "metadata": {},
   "source": [
    "### Threshold Sensitivity Analysis\n",
    "\n",
    "\n",
    "- Get Prediction Probabilities:\n",
    "    - `We use baseline_model.predict_proba(X_test_scaled)[:, 1]` to get the predicted probabilities for the positive class ('Benign').\n",
    "        - `predict_proba` returns an array where each element is a list of probabilities for each class.\n",
    "        - `[:, 1]` selects the probability of the class labeled 1 (which we assigned to 'Benign').\n",
    "    - These probabilities represent the model's confidence that each instance belongs to the 'Benign' class.\n",
    "\n",
    "- Define Thresholds to Evaluate:\n",
    "    - We set up a list of thresholds `[0.3, 0.5, 0.7]` to test.\n",
    "        - `0.5` is the default threshold used by most classifiers.\n",
    "        - By adjusting the threshold, we can control the sensitivity of the model to the positive class.\n",
    "\n",
    "- Adjust Predictions Based on Thresholds:\n",
    "    - For each threshold in the list:\n",
    "        - We compare the predicted probabilities to the threshold: `(y_probs >= thresh)`.\n",
    "        - This creates a boolean array where `True` indicates the probability is greater than or equal to the threshold.\n",
    "        - We convert the boolean array to integers (`0` or `1`) using `.astype(int)`, resulting in the adjusted predictions.\n",
    "\n",
    "- Evaluate the Model:\n",
    "    - Accuracy Score:\n",
    "        - We calculate the accuracy using `accuracy_score(y_test, y_pred_thresh)`.\n",
    "        - This tells us the proportion of correct predictions at the given threshold.\n",
    "    - Classification Report:\n",
    "        - We generate a classification report showing precision, recall, F1-score, and support for both 'Attack' and 'Benign' classes.\n",
    "        - This helps us understand how adjusting the threshold affects these metrics.\n",
    "    - Confusion Matrix:\n",
    "        - We display the confusion matrix using `confusion_matrix(y_test, y_pred_thresh)`.\n",
    "        - It shows the counts of true positives, false positives, true negatives, and false negatives.\n",
    "\n",
    "- Repeat for Each Threshold:\n",
    "    - The loop runs for each threshold `(0.3, 0.5, 0.7)`, allowing us to compare the model's performance at different sensitivity levels.\n",
    "\n",
    "**Why Adjust Thresholds?**\n",
    "- Purpose:\n",
    "    - The default threshold of `0.5` might not be optimal, especially in imbalanced datasets.\n",
    "    - Adjusting the threshold can help balance between precision and recall based on our specific needs.\n",
    "        - Lower Threshold (e.g., `0.3`):\n",
    "            - The model is more likely to predict 'Benign'.\n",
    "            - May increase recall (catch more positive cases) but decrease precision (more false positives).\n",
    "        - Higher Threshold (e.g., `0.7`):\n",
    "            - The model is less likely to predict 'Benign'.\n",
    "            - May increase precision (fewer false positives) but decrease recall (miss more positive cases).\n",
    "\n",
    "- Application:\n",
    "    - In intrusion detection, we might prefer to minimize false negatives (undetected attacks), so we might choose a lower threshold to catch more potential attacks, accepting more false positives as a trade-off.\n",
    "    - By evaluating different thresholds, we can select the one that offers the best balance for our specific objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f37a7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction probabilities\n",
    "y_probs = baseline_model.predict_proba(X_test_scaled)[:, 1]  # Probability of class 'Benign' (label 1)\n",
    "\n",
    "# Define thresholds to evaluate\n",
    "thresholds = [0.3, 0.5, 0.7]\n",
    "\n",
    "for thresh in thresholds:\n",
    "    # Predict based on adjusted threshold\n",
    "    y_pred_thresh = (y_probs >= thresh).astype(int)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred_thresh)\n",
    "    print(f\"Threshold {thresh} - Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Threshold {thresh} - Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred_thresh, target_names=['Attack', 'Benign']))\n",
    "    print(f\"Threshold {thresh} - Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred_thresh))\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5150fec0",
   "metadata": {},
   "source": [
    "### Cross-Validation\n",
    "Explanation:\n",
    "\n",
    "- Why Use Cross-Validation?\n",
    "    - Cross-validation provides a more reliable estimate of the model's performance by evaluating it on multiple subsets of the data.\n",
    "    - It helps prevent overfitting and assesses how well the model might perform on new, unseen data.\n",
    "\n",
    "- Initialize Stratified K-Fold:\n",
    "    - We use `StratifiedKFold` to maintain the same class distribution in each fold as in the original dataset.\n",
    "    - Parameters:\n",
    "        - `n_splits=5`: Splits the data into five folds.\n",
    "        - `shuffle=True`: Randomizes the data before splitting to ensure a good mix.\n",
    "        - `random_state=42`: Sets a seed for reproducibility.\n",
    "\n",
    "- Perform Cross-Validation:\n",
    "    - The `cross_val_score` function evaluates the model using cross-validation.\n",
    "    - Parameters:\n",
    "        - `baseline_model`: The model we want to evaluate.\n",
    "        - `X_train_scaled`: The features from the training set.\n",
    "        - `y_train`: The target variable from the training set.\n",
    "        - `cv=skf`: Specifies the cross-validation strategy to use.\n",
    "        - `scoring='f1'`: Uses the F1 score as the evaluation metric.\n",
    "    - Returns:\n",
    "        - An array cv_scores containing the F1 scores for each fold.\n",
    "\n",
    "By using cross-validation, we gain a better understanding of how our model performs across different subsets of the data, leading to a more robust evaluation than a single train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0dd154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "# Initialize stratified k-fold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Cross-validate the model\n",
    "cv_scores = cross_val_score(baseline_model, X_train_scaled, y_train, cv=skf, scoring='f1')\n",
    "\n",
    "print(f\"Cross-Validation F1 Scores: {cv_scores}\")\n",
    "print(f\"Mean F1 Score: {cv_scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4cf621",
   "metadata": {},
   "source": [
    "# Step 8: Visualizing Results\n",
    "### ROC Curve Analysis\n",
    "We visualize our model's performance using the ROC curve:\n",
    "\n",
    "- Purpose:\n",
    "    - The ROC curve shows how the true positive rate (TPR) relates to the false positive rate (FPR) at different thresholds.\n",
    "    - The AUC summarizes this relationship in a single number.\n",
    "\n",
    "- Steps:\n",
    "    - Get Predicted Probabilities:\n",
    "        - Extract probabilities for the 'Benign' class from the model.\n",
    "    - Calculate ROC Metrics:\n",
    "        - Compute false positive rates, true positive rates, and thresholds.\n",
    "        - Calculate the AUC.\n",
    "    - Plot the ROC Curve:\n",
    "        - Plot TPR vs. FPR.\n",
    "        - Add a diagonal line representing random guessing.\n",
    "        - Include labels, a title, and a legend showing the AUC.\n",
    "\n",
    "- Interpretation:\n",
    "    - The ROC curve helps visualize the trade-off between true positive rate and false positive rate across different thresholds.\n",
    "    - The AUC provides a single metric to evaluate the model's ability to distinguish between the classes.\n",
    "        - An AUC of 1.0 indicates perfect classification; 0.5 suggests no discriminative ability.\n",
    "\n",
    "By analyzing the ROC curve and AUC, we gain deeper insights into our model's performance beyond accuracy, helping us assess how well it can detect attacks while minimizing false alarms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2732280",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get prediction probabilities\n",
    "y_probs_baseline = baseline_model.predict_proba(X_test_scaled)[:, 1]  # Probability of 'Benign'\n",
    "\n",
    "# Calculate ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs_baseline, pos_label=1)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'Baseline Model ROC curve (area = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='grey', lw=1, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Baseline Model')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a5a643",
   "metadata": {},
   "source": [
    "### Compile Performance Metrics\n",
    "We compile and compare the performance metrics of all our models:\n",
    "\n",
    "- Purpose:\n",
    "    - Gather key metrics like Accuracy, Precision, Recall, F1-Score, and AUC for each model.\n",
    "    - Organize them into a DataFrame for easy comparison.\n",
    "\n",
    "- Steps:\n",
    "    - Initialize an Empty List:\n",
    "        - Create metrics to store performance data.\n",
    "    - Collect Metrics for Each Model:\n",
    "        - Baseline Random Forest:\n",
    "            - Calculate metrics using predictions and probabilities.\n",
    "        - Random Forest with Class Weight:\n",
    "            - Compute metrics for the model adjusted for class imbalance.\n",
    "        - Random Forest with SMOTE:\n",
    "            - Gather metrics for the model trained with SMOTE.\n",
    "        - Logistic Regression:\n",
    "            - Collect performance data.\n",
    "        - XGBoost Classifier:\n",
    "            - Compute metrics for the XGBoost model.\n",
    "    - Create a DataFrame:\n",
    "        - Convert the list of metrics into a DataFrame metrics_df.\n",
    "    - Display the Results:\n",
    "        - Print the DataFrame to compare the models side by side.\n",
    "\n",
    "- Why It's Helpful:\n",
    "    - Allows us to see which model performs best according to different metrics.\n",
    "    - Facilitates informed decision-making on which model to choose for our intrusion detection system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ccd18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to store metrics\n",
    "import pandas as pd\n",
    "\n",
    "metrics = []\n",
    "\n",
    "# Baseline Model Metrics\n",
    "metrics.append({\n",
    "    'Model': 'Baseline RandomForest',\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_baseline),\n",
    "    'Precision': precision_score(y_test, y_pred_baseline),\n",
    "    'Recall': recall_score(y_test, y_pred_baseline),\n",
    "    'F1-Score': f1_score(y_test, y_pred_baseline),\n",
    "    'AUC': roc_auc_score(y_test, y_probs_baseline)\n",
    "})\n",
    "\n",
    "# Model with Class Weight Metrics\n",
    "y_probs_class_weight = model_class_weight.predict_proba(X_test_scaled)[:, 1]\n",
    "metrics.append({\n",
    "    'Model': 'RandomForest with Class Weight',\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_class_weight),\n",
    "    'Precision': precision_score(y_test, y_pred_class_weight),\n",
    "    'Recall': recall_score(y_test, y_pred_class_weight),\n",
    "    'F1-Score': f1_score(y_test, y_pred_class_weight),\n",
    "    'AUC': roc_auc_score(y_test, y_probs_class_weight)\n",
    "})\n",
    "\n",
    "# SMOTE Model Metrics\n",
    "y_probs_smote = model_smote.predict_proba(X_test_scaled)[:, 1]\n",
    "metrics.append({\n",
    "    'Model': 'RandomForest with SMOTE',\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_smote),\n",
    "    'Precision': precision_score(y_test, y_pred_smote),\n",
    "    'Recall': recall_score(y_test, y_pred_smote),\n",
    "    'F1-Score': f1_score(y_test, y_pred_smote),\n",
    "    'AUC': roc_auc_score(y_test, y_probs_smote)\n",
    "})\n",
    "\n",
    "# Logistic Regression Metrics\n",
    "y_probs_logreg = model_logreg.predict_proba(X_test_scaled)[:, 1]\n",
    "metrics.append({\n",
    "    'Model': 'Logistic Regression',\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_logreg),\n",
    "    'Precision': precision_score(y_test, y_pred_logreg),\n",
    "    'Recall': recall_score(y_test, y_pred_logreg),\n",
    "    'F1-Score': f1_score(y_test, y_pred_logreg),\n",
    "    'AUC': roc_auc_score(y_test, y_probs_logreg)\n",
    "})\n",
    "\n",
    "# XGBoost Classifier Metrics\n",
    "y_probs_xgb = model_xgb.predict_proba(X_test_scaled)[:, 1]\n",
    "metrics.append({\n",
    "    'Model': 'XGBoost Classifier',\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_xgb),\n",
    "    'Precision': precision_score(y_test, y_pred_xgb),\n",
    "    'Recall': recall_score(y_test, y_pred_xgb),\n",
    "    'F1-Score': f1_score(y_test, y_pred_xgb),\n",
    "    'AUC': roc_auc_score(y_test, y_probs_xgb)\n",
    "})\n",
    "\n",
    "# Create DataFrame\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be99e6a4",
   "metadata": {},
   "source": [
    "### Plotting Performance Metrics\n",
    "\n",
    "Shows a visualization of the different models with each respective metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e58819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the metrics\n",
    "import seaborn as sns\n",
    "\n",
    "# Melt the DataFrame for easier plotting\n",
    "metrics_melted = metrics_df.melt(id_vars='Model', var_name='Metric', value_name='Value')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=metrics_melted, x='Metric', y='Value', hue='Model')\n",
    "plt.title('Model Comparison')\n",
    "plt.ylabel('Score')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
