{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b76caf6",
   "metadata": {},
   "source": [
    "IMPORTANT NOTE: The following code and explanation was generated via ChatGPT o1-preview model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a60cfd",
   "metadata": {},
   "source": [
    "Step 2: Import Necessary Libraries\n",
    "\n",
    "In your Jupyter Notebook, start by importing the libraries we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "816a5cf1-b89d-4e39-b76b-9df46d1f3455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Add any other imports you may have\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7ee753",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "    pandas: For data manipulation and analysis.\n",
    "    numpy: For numerical computations.\n",
    "    os: For interacting with the operating system.\n",
    "    matplotlib.pyplot and seaborn: For data visualization.\n",
    "    sklearn: For machine learning algorithms and evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Load the CSV Files into DataFrames\n",
    "3.1 Get a List of CSV Files\n",
    "\n",
    "\n",
    "We'll use the os library to get all CSV files in the dataset folder.\n",
    "\n",
    "Explanation:\n",
    "    glob.glob() retrieves all files matching the pattern.\n",
    "    This code lists all CSV files we will load."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a73d062",
   "metadata": {},
   "source": [
    "3.2 Load the CSV Files\n",
    "\n",
    "Now, we'll load each CSV file into a DataFrame and store them in a list.\n",
    "\n",
    "Explanation:\n",
    "    We read each CSV file and append it to the dataframes list.\n",
    "    encoding='latin1' is used to handle any special characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0b240b",
   "metadata": {},
   "source": [
    "Step 4: Combine the DataFrames\n",
    "\n",
    "We'll concatenate all DataFrames into one large DataFrame.\n",
    "\n",
    "Explanation:\n",
    "    pd.concat() combines all DataFrames vertically.\n",
    "    ignore_index=True resets the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b178ec18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your dataset folder\n",
    "data_path = 'dataset/'\n",
    "\n",
    "# Get a list of all CSV files in the dataset folder\n",
    "csv_files = glob.glob(os.path.join(data_path, '*.csv'))\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file, encoding='utf-8')  # Adjust encoding if necessary\n",
    "    # Strip whitespace from column names\n",
    "    df.columns = df.columns.str.strip()\n",
    "    dataframes.append(df)\n",
    "    print(f\"Loaded {file} with shape {df.shape}\")\n",
    "\n",
    "# Combine all DataFrames\n",
    "data = pd.concat(dataframes, ignore_index=True)\n",
    "print(f\"Combined DataFrame shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b3c8cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the unidentified character '�' with a hyphen '-' in the 'Label' column\n",
    "data['Label'] = data['Label'].str.replace('�', '-', regex=False)\n",
    "\n",
    "# Optionally, strip any leading/trailing whitespace from the 'Label' column\n",
    "data['Label'] = data['Label'].str.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398cd8fd",
   "metadata": {},
   "source": [
    "Step 5: Explore the Data\n",
    "    5.1 View the First Few Rows\n",
    "\n",
    "Explanation:\n",
    "    Displays the first five rows of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f202743",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7db4b8c",
   "metadata": {},
   "source": [
    "5.2 Get DataFrame Information\n",
    "Explanation:\n",
    "    Shows data types and counts of non-null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d7ab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a216d232",
   "metadata": {},
   "source": [
    "5.3 Check for Missing Values\n",
    "Explanation:\n",
    "    Identifies columns with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2b3646",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = data.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4830c934",
   "metadata": {},
   "source": [
    "Step 6: Preprocess the Data\n",
    "6.1 Handle Missing Values\n",
    "    Dataset is largely complete, will not need to drop columns with too many missing values or create synthetic data via mean\n",
    "    Skip Step 6.1\n",
    "\n",
    "6.2 Convert Non-Numeric Columns to Numeric\n",
    "\n",
    "Identify any non-numeric columns that need to be converted.\n",
    "Possible non-numeric columns could be 'Label' or others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26a6618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify non-numeric columns\n",
    "non_numeric_cols = data.select_dtypes(include=['object']).columns\n",
    "print(\"Non-numeric columns:\", non_numeric_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c79783",
   "metadata": {},
   "source": [
    "6.3 Encode Categorical Variables\n",
    "    6.3.1 Encode the Target Variable\n",
    "\n",
    "Assuming the target variable is 'Label'.\n",
    "\n",
    "Explanation:\n",
    "    This shows the different classes in the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9aacce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values in the Label column\n",
    "print(data['Label'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6639c70",
   "metadata": {},
   "source": [
    "1.1 Encode Labels for Binary Classification\n",
    "\n",
    "In this step, we'll map all attack types to `Attack` and benign traffic to `Benign`. We'll then encode these labels into numerical values.\n",
    "\n",
    "The Label Values are as follows: \n",
    "    ['BENIGN' 'Infiltration' 'Bot' 'PortScan' 'DDoS' 'FTP-Patator',\n",
    "     'SSH-Patator' 'DoS slowloris' 'DoS Slowhttptest' 'DoS Hulk', \n",
    "     'DoS GoldenEye' 'Heartbleed' 'Web Attack - Brute Force',\n",
    "     'Web Attack - XSS' 'Web Attack - Sql Injection']\n",
    "\n",
    "Explanation:\n",
    "    We create a new column `Label_binary` where all attack types are mapped to `Attack` and benign traffic to `Benign`.\n",
    "    `LabelEncoder` is used to convert these categorical labels into numerical values (0 and 1).\n",
    "    We print the label mapping to verify that `Attack` and `Benign` are correctly encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6839d165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels for binary classification\n",
    "data['Label_binary'] = data['Label'].apply(lambda x: 'Benign' if x == 'BENIGN' else 'Attack')\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "data['Label_encoded'] = le.fit_transform(data['Label_binary'])\n",
    "\n",
    "# Display label encoding mapping\n",
    "label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(\"Label Encoding Mapping:\")\n",
    "for label, encoding in label_mapping.items():\n",
    "    print(f\"{label}: {encoding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa05e548",
   "metadata": {},
   "source": [
    "1.2 Prepare Features (X) and Target (y)\n",
    "\n",
    "We'll separate our dataset into features and target variables.\n",
    "\n",
    "Explanation:\n",
    "    `X` contains all the features used for training, excluding the original and encoded labels.\n",
    "    `y` is our target variable containing the encoded labels (`0` for **Attack**, `1` for **Benign**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4be95c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features (drop unnecessary columns)\n",
    "X = data.drop(['Label', 'Label_binary', 'Label_encoded'], axis=1, errors='ignore')\n",
    "\n",
    "# Target variable\n",
    "y = data['Label_encoded']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71773bf",
   "metadata": {},
   "source": [
    "1.3 Handle Non-Numeric Features\n",
    "\n",
    "We need to ensure all features are numeric.\n",
    "\n",
    "Explanation:\n",
    "    We check for any non-numeric columns in `X`.\n",
    "    If non-numeric columns are found, we apply one-hot encoding to convert them into numeric format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a238de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify non-numeric columns\n",
    "non_numeric_cols = X.select_dtypes(include=['object']).columns\n",
    "print(\"Non-numeric columns:\", non_numeric_cols.tolist())\n",
    "\n",
    "# Encode non-numeric features\n",
    "if len(non_numeric_cols) > 0:\n",
    "    X = pd.get_dummies(X, columns=non_numeric_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139b541a",
   "metadata": {},
   "source": [
    "1.4 Split Data into Training and Testing Sets\n",
    "\n",
    "We'll split our data into training and testing sets to evaluate model performance.\n",
    "\n",
    "Explanation:\n",
    "    We use `train_test_split` to split the data.\n",
    "    `test_size=0.2` reserves 20% of the data for testing.\n",
    "    `stratify=y` ensures the class distribution is consistent in both training and testing sets.\n",
    "    `random_state=42` ensures reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3152a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data with stratification to maintain class balance\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df30b89c",
   "metadata": {},
   "source": [
    "Step 2: Handling Infinite and Missing Values\n",
    "\n",
    "2.1 Replace Infinite Values and Handle Missing Data\n",
    "\n",
    "We need to replace infinite values and handle any missing data in our features.\n",
    "\n",
    "Explanation:\n",
    "    Infinite values are replaced with `NaN` to handle them appropriately.\n",
    "    We check for missing values in `X_train` and `X_test`.\n",
    "    `SimpleImputer` is used to fill missing values with the mean of each feature, calculated from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a73f31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "X_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Check for NaN values\n",
    "print(\"Checking for NaN values in X_train and X_test:\")\n",
    "print(f\"X_train contains NaN values: {X_train.isnull().values.any()}\")\n",
    "print(f\"X_test contains NaN values: {X_test.isnull().values.any()}\")\n",
    "\n",
    "# Impute missing values with the mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Fit on X_train and transform both X_train and X_test\n",
    "X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0366b6",
   "metadata": {},
   "source": [
    "Step 3: Feature Scaling\n",
    "3.1 Apply StandardScaler\n",
    "\n",
    "We scale the features to normalize the data.\n",
    "\n",
    "Explanation:\n",
    "    `StandardScaler` standardizes features by removing the mean and scaling to unit variance.\n",
    "    We fit the scaler on `X_train` and transform both `X_train` and `X_test` to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e878cfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc29b74",
   "metadata": {},
   "source": [
    "Step 4: Addressing Class Imbalance\n",
    "4.1 Oversample Minority Class using SMOTE\n",
    "\n",
    "We address class imbalance by oversampling the minority class ('Attack') using **SMOTE** (Synthetic Minority Oversampling Technique).\n",
    "\n",
    "Explanation:\n",
    "    `SMOTE` generates synthetic samples of the minority class to balance the dataset.\n",
    "    `sampling_strategy='auto'` balances all classes to the number of samples in the majority class.\n",
    "    We apply `SMOTE` only to the training data to avoid data leakage.\n",
    "    We print class distributions before and after resampling to verify the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedddfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Model: RandomForestClassifier without class weights\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Train the baseline model\n",
    "baseline_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "baseline_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_baseline = baseline_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred_baseline)\n",
    "print(f\"Baseline Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"Baseline Model Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_baseline, target_names=['Attack', 'Benign']))\n",
    "\n",
    "print(\"Baseline Model Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e301f4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with class_weight='balanced'\n",
    "model_class_weight = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
    "model_class_weight.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_class_weight = model_class_weight.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred_class_weight)\n",
    "print(f\"Model with Class Weight Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"Model with Class Weight Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_class_weight, target_names=['Attack', 'Benign']))\n",
    "\n",
    "print(\"Model with Class Weight Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_class_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2df27fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Define the SMOTE object\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Resample the training data\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(\"After SMOTE oversampling:\")\n",
    "print(f\"Original y_train distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Resampled y_train distribution: {np.bincount(y_train_resampled)}\")\n",
    "\n",
    "# Train the model on resampled data\n",
    "model_smote = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model_smote.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_smote = model_smote.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred_smote)\n",
    "print(f\"Model with SMOTE Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"Model with SMOTE Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_smote, target_names=['Attack', 'Benign']))\n",
    "\n",
    "print(\"Model with SMOTE Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_smote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c712cdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize and train the model\n",
    "model_logreg = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)\n",
    "model_logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_logreg = model_logreg.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred_logreg)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"Logistic Regression Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_logreg, target_names=['Attack', 'Benign']))\n",
    "\n",
    "print(\"Logistic Regression Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_logreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9edcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the XGBoost classifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Calculate the scale_pos_weight parameter\n",
    "from collections import Counter\n",
    "counter = Counter(y_train)\n",
    "ratio = counter[1] / counter[0]\n",
    "print(f\"Scale_pos_weight ratio: {ratio}\")\n",
    "\n",
    "# Initialize and train the model\n",
    "model_xgb = XGBClassifier(scale_pos_weight=ratio, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "model_xgb.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgb = model_xgb.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred_xgb)\n",
    "print(f\"XGBoost Classifier Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"XGBoost Classifier Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb, target_names=['Attack', 'Benign']))\n",
    "\n",
    "print(\"XGBoost Classifier Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_xgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f37a7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction probabilities\n",
    "y_probs = baseline_model.predict_proba(X_test_scaled)[:, 1]  # Probability of class 'Benign' (label 1)\n",
    "\n",
    "# Define thresholds to evaluate\n",
    "thresholds = [0.3, 0.5, 0.7]\n",
    "\n",
    "for thresh in thresholds:\n",
    "    # Predict based on adjusted threshold\n",
    "    y_pred_thresh = (y_probs >= thresh).astype(int)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred_thresh)\n",
    "    print(f\"Threshold {thresh} - Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Threshold {thresh} - Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred_thresh, target_names=['Attack', 'Benign']))\n",
    "    print(f\"Threshold {thresh} - Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred_thresh))\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0dd154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "# Initialize stratified k-fold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Cross-validate the model\n",
    "cv_scores = cross_val_score(baseline_model, X_train_scaled, y_train, cv=skf, scoring='f1')\n",
    "\n",
    "print(f\"Cross-Validation F1 Scores: {cv_scores}\")\n",
    "print(f\"Mean F1 Score: {cv_scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2732280",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get prediction probabilities\n",
    "y_probs_baseline = baseline_model.predict_proba(X_test_scaled)[:, 1]  # Probability of 'Benign'\n",
    "\n",
    "# Calculate ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs_baseline, pos_label=1)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'Baseline Model ROC curve (area = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='grey', lw=1, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Baseline Model')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ccd18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to store metrics\n",
    "import pandas as pd\n",
    "\n",
    "metrics = []\n",
    "\n",
    "# Baseline Model Metrics\n",
    "metrics.append({\n",
    "    'Model': 'Baseline RandomForest',\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_baseline),\n",
    "    'Precision': precision_score(y_test, y_pred_baseline),\n",
    "    'Recall': recall_score(y_test, y_pred_baseline),\n",
    "    'F1-Score': f1_score(y_test, y_pred_baseline),\n",
    "    'AUC': roc_auc_score(y_test, y_probs_baseline)\n",
    "})\n",
    "\n",
    "# Model with Class Weight Metrics\n",
    "y_probs_class_weight = model_class_weight.predict_proba(X_test_scaled)[:, 1]\n",
    "metrics.append({\n",
    "    'Model': 'RandomForest with Class Weight',\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_class_weight),\n",
    "    'Precision': precision_score(y_test, y_pred_class_weight),\n",
    "    'Recall': recall_score(y_test, y_pred_class_weight),\n",
    "    'F1-Score': f1_score(y_test, y_pred_class_weight),\n",
    "    'AUC': roc_auc_score(y_test, y_probs_class_weight)\n",
    "})\n",
    "\n",
    "# SMOTE Model Metrics\n",
    "y_probs_smote = model_smote.predict_proba(X_test_scaled)[:, 1]\n",
    "metrics.append({\n",
    "    'Model': 'RandomForest with SMOTE',\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_smote),\n",
    "    'Precision': precision_score(y_test, y_pred_smote),\n",
    "    'Recall': recall_score(y_test, y_pred_smote),\n",
    "    'F1-Score': f1_score(y_test, y_pred_smote),\n",
    "    'AUC': roc_auc_score(y_test, y_probs_smote)\n",
    "})\n",
    "\n",
    "# Logistic Regression Metrics\n",
    "y_probs_logreg = model_logreg.predict_proba(X_test_scaled)[:, 1]\n",
    "metrics.append({\n",
    "    'Model': 'Logistic Regression',\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_logreg),\n",
    "    'Precision': precision_score(y_test, y_pred_logreg),\n",
    "    'Recall': recall_score(y_test, y_pred_logreg),\n",
    "    'F1-Score': f1_score(y_test, y_pred_logreg),\n",
    "    'AUC': roc_auc_score(y_test, y_probs_logreg)\n",
    "})\n",
    "\n",
    "# XGBoost Classifier Metrics\n",
    "y_probs_xgb = model_xgb.predict_proba(X_test_scaled)[:, 1]\n",
    "metrics.append({\n",
    "    'Model': 'XGBoost Classifier',\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_xgb),\n",
    "    'Precision': precision_score(y_test, y_pred_xgb),\n",
    "    'Recall': recall_score(y_test, y_pred_xgb),\n",
    "    'F1-Score': f1_score(y_test, y_pred_xgb),\n",
    "    'AUC': roc_auc_score(y_test, y_probs_xgb)\n",
    "})\n",
    "\n",
    "# Create DataFrame\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e58819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the metrics\n",
    "import seaborn as sns\n",
    "\n",
    "# Melt the DataFrame for easier plotting\n",
    "metrics_melted = metrics_df.melt(id_vars='Model', var_name='Metric', value_name='Value')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=metrics_melted, x='Metric', y='Value', hue='Model')\n",
    "plt.title('Model Comparison')\n",
    "plt.ylabel('Score')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
